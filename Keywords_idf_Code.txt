from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import math

stop=set(stopwords.words('english'))
stri=""


import os
  
# Folder Path
path = r"path of the folder where code is executed"
  
# Change the directory
os.chdir(path)

key =list()
# iterate through all file
for file in os.listdir():
    # Check whether file is in text format or not
    if file.endswith(".txt"):
        file_path = f"{path}\{file}"
        with open(file_path) as f:
            data =f.readlines()
            for x in data:
                val=x.strip()
                stri=stri+val
            tok=word_tokenize(stri)
            ans=list()
            for i in range(0,len(tok)):
                if tok[i].lower() in stop:
                    pass
                else:
                    ans.append(tok[i])
            none=""
            ans = list(map(lambda x: x.replace('\n',none).replace('\\n',none).replace(',',none),ans))
            for i in range(len(ans)-1):
                ans[i]=ans[i].lower()
                if ans[i]=='' or ans[i]=='.' or ans[i].isalpha() == False:
                    pass
                else:
                    key.append(ans[i])
            ans.clear()
            stri=""

key_final=list()

for i in key:
    if i in key_final:
        pass
    else:
        key_final.append(i)

key_final.sort()

# with open("keywords_final.txt","a+") as f:
#     f.write('\n'.join(key_final))


nt = []
for i in key_final:
    nt.append(key.count(i))

idf =[]
cnt = 0
for i in key_final:
    idf.append((math.log10(1539/float(nt[cnt])))+1)
    cnt +=1
with open("idf_final.txt","a+") as f:
        for x in idf:
            f.write("%s\n" % x)